<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
<head>
      <link rel="stylesheet" href="styles.css">
      <meta charset="UTF-8">
      <meta name="keywords" content="3D Shape learning">
      <meta name="author" content="Stefan Stojanov, Anh Thai">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
 
<div class="header">
  <h1>SyncWISE: Window Induced Shift Estimation for Synchronization of Video and Accelerometry from Wearable Sensors</h1>
  <p class="authors">
        <a href="https://sites.google.com/view/yunzhang/home">Yun C. Zhang</a><sup>1</sup>*,
        <a href="https://zsb87.github.io/">Shibo Zhang</a><sup>2</sup>*,
        <a href="https://aptx4869lm.github.io/">Miao Liu</a><sup>1</sup>,
        Elyse Daly<sup>2</sup>,
        Samuel Battalio<sup>2</sup>,
        <a href="https://md2k.org/santosh">Santosh Kumar</a><sup>3</sup>,
        <a href="https://www.feinberg.northwestern.edu/faculty-profile/az/profile.html?xid=16136">Bonnie Spring</a><sup>2</sup>,
        <a href="https://rehg.org/">James M. Rehg</a><sup>1</sup>,
        <a href="http://www.nalshurafa.com/">Nabil Alshurafa</a><sup>2</sup>,
  </p>
  <p class="institution">
        <sup>1</sup>Georgia Institute of Technology,
        <sup>2</sup>Norhwestern University,
        <sup>3</sup>University of Memphis,
  </p>
  <span class="footnote">
    * equal contribution
  </p>
</div>     

<div class="links">

    <div class="gallery">
      <a target="_blank" href="https://github.com/HAbitsLab/SyncWISE/">
	<img src="img/code_img.png" alt="code">
      </a>
      <div class="desc">Code</div>
    </div>
    
    <div class="gallery">
      <a target="_blank" href="https://drive.google.com/file/d/1jZl6hark38PxK37jiJv0F5l06IxZTNn5/view?usp=sharing">
	<img src="img/paper_img.png" alt="arxiv link">
      </a>
      <div class="desc">Paper <a href="/SyncWISE/bibtex/syncwise.bib">[BibTex]</a></div>
    </div>

</div>

<div class="content">

<!-- <h1> Abstract </h1>
<p>
A key challenge in single image 3D shape reconstruction is to ensure that deep models designed for single-view shape prediction can generalize to shapes which were not part of the training set. We find that such generalization to unseen categories of objects is a function of both architecture design and object pose representation during training. This paper introduces SDFNet, a novel shape prediction architecture and a training approach which supports effective generalization. We provide an extensive investigation of the factors that influence generalization accuracy and its measurement, ranging from the consistent use of 3D shape metrics to the choice of rendering parameters. We show that SDFNet provides state-of-the-art performance on seen and unseen shapes relative to existing baseline methods. We provide the first large-scale experimental evaluation of generalization performance on the complete ShapeNetCore.v2 dataset. 
</p> -->



    <p>
      Our proposed architecture SDFNet is able to successfuly reconstruct the shape from a single 
      image of object shape categories seen during training as well as new, unseen object categories. 
      SDFNet is trained to predict SDF values in the same pose as the input image without requiring knowledge of camera parameters or object pose at test time.
      Qualitative results show superiority compared to baselines GenRe <a href="#GenRe">[1]</a> and OccNet <a href="#OccNet">[2]</a></p>
      
      <div class="gifs">
          <div class="desc_vert">Seen Class</div>
          <div class="container_gif">
              
              <div class="gallery">
                  <img class="gif" src="gif/GIF/occnet_sdfnet/table.png" >
                  <div class="desc">Input</div>
                </div>
            <div class="gallery">
                
              <img class="gif" src="gif/GIF/occnet_sdfnet/table_sdfnet_pred_gif.gif" >
              <div class="desc">SDFNet</div>
            </div>
            
            <div class="gallery">
              <img class="gif" src="gif/GIF/occnet_sdfnet/table_occnet_gif.gif" >
              <div class="desc">OccNet</div>
            </div>
      
            <div class="gallery">
                <img class="gif" src="gif/GIF/occnet_sdfnet/table_gt_gif.gif" >
                <div class="desc">Ground Truth</div>
              </div>

              <div class="gallery">
                  <img class="gif" src="gif/GIF/genre/table_input.png" >
                  <div class="desc">Input</div>
                </div>
            
              <div class="gallery">
                  <img class="gif" src="gif/GIF/genre/table_pred_gif.gif" >
                  <div class="desc">GenRe</div>
                </div>
          
                <div class="gallery">
                    <img class="gif" src="gif/GIF/genre/table_gt_gif.gif" >
                    <div class="desc">Ground Truth</div>
                  </div>
            
            
          </div>
      </div>

      <div class="gifs">
          <div class="desc_vert">Novel Class</div>
          <div class="container_gif">
              
              <div class="gallery">
                  <img class="gif" src="gif/GIF/occnet_sdfnet/mailbox.png" >
                  <div class="desc">Input</div>
                </div>
            <div class="gallery">
                
              <img class="gif" src="gif/GIF/occnet_sdfnet/mailbox_sdfnet_pred.gif" >
              <div class="desc">SDFNet</div>
            </div>
            
            <div class="gallery">
              <img class="gif" src="gif/GIF/occnet_sdfnet/mailbox_occnet_pred.gif" >
              <div class="desc">OccNet</div>
            </div>
      
            <div class="gallery">
                <img class="gif" src="gif/GIF/occnet_sdfnet/mailbox_gt.gif" >
                <div class="desc">Ground Truth</div>
              </div>

              <div class="gallery">
                  <img class="gif" src="gif/GIF/genre/mailbox_cropped.png" >
                  <div class="desc">Input</div>
                </div>
            
              <div class="gallery">
                  <img class="gif" src="gif/GIF/genre/mailbox_pred_gif.gif" >
                  <div class="desc">GenRe</div>
                </div>
          
                <div class="gallery">
                    <img class="gif" src="gif/GIF/genre/mailbox_gt_gif.gif" >
                    <div class="desc">Ground Truth</div>
                  </div>
            
            
          </div>
        </div>

        <div class="gifs">
            <div class="desc_vert">Novel Class</div>
            <div class="container_gif">
                
                <div class="gallery">
                    <img class="gif" src="gif/GIF/occnet_sdfnet/watch.png" >
                    <div class="desc">Input</div>
                  </div>
              <div class="gallery">
                  
                <img class="gif" src="gif/GIF/occnet_sdfnet/watch_sdfnet_pred.gif" >
                <div class="desc">SDFNet</div>
              </div>
              
              <div class="gallery">
                <img class="gif" src="gif/GIF/occnet_sdfnet/watch_occnet_pred.gif" >
                <div class="desc">OccNet</div>
              </div>
        
              <div class="gallery">
                  <img class="gif" src="gif/GIF/occnet_sdfnet/watch_gt.gif" >
                  <div class="desc">Ground Truth</div>
                </div>

                <div class="gallery">
                    <img class="gif" src="gif/GIF/genre/watch_input.png" >
                    <div class="desc">Input</div>
                  </div>
        
                <div class="gallery">
                    <img class="gif" src="gif/GIF/genre/genre_watch.gif" >
                    <div class="desc">GenRe</div>
                  </div>
            
                <div class="gallery">
                    <img class="gif" src="gif/GIF/genre/watch_gt_gif.gif" >
                    <div class="desc">Ground Truth</div>
                  </div>
              
              
            </div>
          </div>

          <div class="gifs">
              <div class="desc_vert">Novel Class</div>
              <div class="container_gif">
                  
                  <div class="gallery">
                      <img class="gif" src="gif/GIF/occnet_sdfnet/sink.png" >
                      <div class="desc">Input</div>
                    </div>
                <div class="gallery">
                    
                  <img class="gif" src="gif/GIF/occnet_sdfnet/sink_sdfnet_pred.gif" >
                  <div class="desc">SDFNet</div>
                </div>
                
                <div class="gallery">
                  <img class="gif" src="gif/GIF/occnet_sdfnet/sink_occnet_pred.gif" >
                  <div class="desc">OccNet</div>
                </div>
          
                <div class="gallery">
                    <img class="gif" src="gif/GIF/occnet_sdfnet/sink_gt.gif" >
                    <div class="desc">Ground Truth</div>
                  </div>
                
                  <div class="gallery">
                      <img class="gif" src="gif/GIF/genre/sink_cropped.png" >
                      <div class="desc">Input</div>
                    </div>
          
                  <div class="gallery">
                      <img class="gif" src="gif/GIF/genre/sink_pred_gif.gif" >
                      <div class="desc">GenRe</div>
                    </div>
              
                  <div class="gallery">
                      <img class="gif" src="gif/GIF/genre/sink_gt_gif.gif" >
                      <div class="desc">Ground Truth</div>
                    </div>
                
                
              </div>
            </div>
    
<div class="desc">Qualitative results of SDFNet, OccNet VC and GenRe on seen and unseen classes of 2 DOF ShapeNetCore.v2</div>

<!-- <br><br> -->
<h1> Generalizing to Seen and Unseen Shapes </h1>
<p>SDFNet is able to capture fine-grained details on the surface of seen classes (airplane example below) and infer occluded 
  surfaces of unseen categories (bathtub and camera). By explicitly estimating 2.5D sketches, SDFNet can capture concave surfaces (bathtub) 
  and protruding surfaces (camera lens).
</p>
<div class="gifs">
    <div class="container">
        <div class="desc">Seen Class</div>
      <div class="gallery">
          
        <img class="gif" src="gif/airplane_PRED.gif" >
        <div class="desc">Predicted</div>
      </div>
      
      <div class="gallery">
        <img class="gif" src="gif/airplane_GT.gif" >
        <div class="desc">Ground Truth</div>
      </div>
      
    </div>
    <div class="container">
        <div class="desc">Unseen Class</div>
      <div class="gallery">
        
        <img class="gif" src="gif/tub_PRED.gif" >
        <div class="desc">Predicted</div>
      </div>
    
      <div class="gallery">
        
        <img class="gif" src="gif/tub_GT.gif" >
        <div class="desc">Ground Truth</div>
      </div>
      
    </div>

    <div class="container">
      <div class="desc">Unseen Class</div>
      <div class="gallery">
        <img class="gif" src="gif/camera_PRED.gif" >
        <div class="desc">Predicted</div>
      </div>
      
      <div class="gallery">
        
        <img class="gif" src="gif/camera_GT.gif" >
        <div class="desc">Ground Truth</div>
      </div>
      
    </div>
    
</div>
<div class="desc">SDFNet reconstruction performance on classes seen during training and novel classes not seen during training</div>

</div>

<div class="content">


<h1> Viewer Centered Training Affects Generalization</h1>
<p>
  When evaluated on 3 Degree-of-Freedom Viewer Centered (3 DOF VC)&mdash;object pose varies along azimuth, elevation and tilt,
  our empirical findings show marginal decrease in performance between seen and unseen classes for the 3 DOF VC model. 
  This is new evidence that it is possible to learn a general shape representation with correct depth estimation and 3 DOF VC training.
  </p>
<center><img src="img/representation_img.png" ></center>
<div class="desc"> Quantitative evaluation of 3 DOF VC shows high performance on both seen and unseen categories. Model is trained on ground truth depth and normal images.</div>

<h1> Generalization Across Different Datasets </h1>
<center><img src="img/abc_shapenet_img.png" ></center>
<div class="desc"> Sample images of our renders of the four most common ShapeNet categories and of objects from ABC. It is evident that the two datasets have different shape properties.</div>

<p>
To further test the generalization ability of SDFNet, we train it one one shape dataset and test it on a significantly different shape dataset. Our findings show that when trained on ABC and tested on the 42 unseen categories of ShapeNet, 3 DOF VC SDFNet obtains comparable performance to SDFNet trained on the 13 ShapeNet categories. SDFNet trained on ShapeNet performs relatively worse when tested on ABC.
</p>
<center><img src="img/shapenet_abc_PRED_img.png" ></center>
<div class="desc"> Qualitative comparison of models trained on ABC and tested on ShapeNet and vice-versa. Note the good reconstruction quality on the occluded part of the object.</div>

<h1> Citation  </h1>
<p>Bibliography information of this work:</p>
<p>
Yun C. Zhang, Shibo Zhang, Miao Liu, Elyse Daly, Samuel Battalio, Santosh Kumar, Bonnie Spring, James M. Rehg, and Nabil Alshurafa. 2020. SyncWISE: Window Induced Shift Estimation for Synchronization of Video and Accelerometry from Wearable Sensors. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 4, 3, Article 107 (September 2020), 26 pages. https://doi.org/10.1145/3411824
</p>

<h1> Contact  </h1>
<p>Bibliography information of this work:</p>
<p>
For questions about paper, please contact yzhang467 at gatech dot edu
</p>

</div>